

### 1. Overview 

*code repo* https://github.com/langchain-ai/rag-from-scratch

*slides* : https://docs.google.com/presentation/d/1C9IaAwHoWcc4RSTqo-pCoN3h0nCgqV2JEYZUJunv_9Q/edit?usp=sharing

from video (1-4) it's the basics of how RAG WORKS and why it is needed ?

1. **why it's needed** : llm's are trained on a lot of data, but they don't have enough context on your own private data to help you. for example, if i ask how much square feet my house is - it won't know because it hasn't seen the data
2. **how it works** : so, to solve this problem we use retrieval augmented generation 
    - *retrieval* : retrieves a set of documents that are most suitable to answer your question using similarity search in vector space
    - *augmented* : your questions is augmented (made better / or something is added to it) to make it full of context, from which llm can find the answer
    - *generation* : the generation is the last step in which your context is taken into account and then result is generated by the llm
      
3. **technical details and steps** : a whole RAG Setup consists of the following steps
   - **ingestion** : the source data is ingested into a database and collected that is relevant to the questions your user can ask, for example : your company's SOP regarding client calls, as the document is updated regularly, it is ingested and updated in the rag pipeline too at regular intervals
   - **indexing** : the text from the document is parsed, then split in to small chunks of data, for better and efficient retrieval, and then it is converted into vectors - and stored into a vector database - why we use vectors?  - because in a vector space, similar vectors will be closer to each other, and hence we can use algorithms such as nearest neighbour and etc, to get the most relevent doc to our questions
   - **retrieval** : this is the step in which the question in converted into vector - and then a similarity search is done between our ingested documents and the context and some documents with most similarity are retrieved
   - **generation** : the context retrieved is added to the prompt and sent to the llm, the llm then uses the context to answer your question

#### I am using Langchain as the framework of my choice. also, this overview is very high-level overview without any maths involved, which you probably won't even need but it's good to know nvthls

---


  --- ***advanced topics starts*** --- 

### 2. Query Translation

#### - Multi Query

*slides* : https://docs.google.com/presentation/d/15pWydIszbQG3Ipur9COfTduutTZm6ULdkkyX-MNry8I/edit#slide=id.g268cd4ba153_0_0


1. **What is query translation** : it means changing/improving the user query using different methods
2. **why is is needed** : so, what happens is when a user asks a question, we retrieve a similar set of documents from our store using simple distance-search or nearest neighobour search, but this has a disadvantage - what if the user query is poorly written, then the retrieved context won't be as good as the retriever can't understand the questions, it just sees the distance
3. **how is this solved** : so, there are many solutions, but now we discuss multi query, other ones we will discuss in later sections - we use an extra query translation llm in our chain before retrieval, to give it the question and we ask it to generate 5 different questions related to the original one, from different perspectives. 
   
   once, we have 5 questions, we retrieve context from these 5 questions and then take the union of the contexts
   
   the unified context is then passed to the llm and answer is retrieved, it is shown to improve results by a lot

####  - - - some papers on query translation techniques :

- https://arxiv.org/abs/2403.00067v1

---

#### - RAG Fusion

